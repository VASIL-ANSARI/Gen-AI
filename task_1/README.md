# ğŸš€ Web Crawler & Ingestion Pipeline â€“ Task 1  
A production-ready Python-based system that crawls any website, extracts useful content, converts it into clean Markdown files, and prepares the dataset for RAG or downstream ingestion.

## ğŸ“Œ Features Implemented (Task 1 Completed)

### âœ… Website Crawling
- Crawls internal pages within the same domain  
- Extracts links and prevents infinite loops  
- Configurable depth & rate limiting

### âœ… Sitemap Parsing
- Auto-detects and parses sitemap.xml  
- Falls back to crawling if sitemap fails

### âœ… HTML â†’ Markdown Conversion
- Removes scripts, styles, navbars, footers  
- Converts cleaned HTML into structured Markdown  
- Saves files using safe filenames

### âœ… Logging
- All events stored in crawler.log  
- Includes crawl attempts, failures, saved pages

### âœ… Configurable Pipeline
Settings stored in config.yaml

## ğŸ›  Installation
```
pip install -r requirements.txt
```

## â–¶ï¸ How to Run
```
python crawler.py
```

## ğŸ“ Output
Markdown files saved in:
```
scraped_pages/
```

## ğŸ¯ Final Outcome
A fully working ingestion pipeline ready for RAG systems.
